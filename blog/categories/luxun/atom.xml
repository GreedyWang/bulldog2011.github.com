<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: luxun | Abstraction Builder]]></title>
  <link href="http://bulldog2011.github.com/blog/categories/luxun/atom.xml" rel="self"/>
  <link href="http://bulldog2011.github.com/"/>
  <updated>2013-03-29T19:53:16+08:00</updated>
  <id>http://bulldog2011.github.com/</id>
  <author>
    <name><![CDATA[Bulldog]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[the architecture and design of a publish subscribe messaging system tailored for big data collecting and analytics]]></title>
    <link href="http://bulldog2011.github.com/blog/2013/03/27/the-architecture-and-design-of-a-pub-sub-messaging-system/"/>
    <updated>2013-03-27T16:30:00+08:00</updated>
    <id>http://bulldog2011.github.com/blog/2013/03/27/the-architecture-and-design-of-a-pub-sub-messaging-system</id>
    <content type="html"><![CDATA[<h1>Overview</h1>

<p>With the advent of big data era, we are facing more and more challenges from big data collecting and analytics requirements. Typical big data or activity stream includes but not limited to:</p>

<blockquote><ul>
<li>Logs generated by frontend applications or backend services</li>
<li>User behavior data</li>
<li>Application or system performance trace</li>
<li>Business, application or system metrics data.</li>
<li>Events that need immediate action.</li>
</ul>
</blockquote>

<p>The Luxun messaging system is just tailored for big data collecting and analytics scenario, following are the main design objectives of Luxun messaging system:</p>

<!--more-->


<blockquote><ul>
<li><strong><em>Fast and High-Throughput</em></strong> : This is the top priority feature, without this capability, the system will be easily overwhelmed by flooding data continuously generated by hundreds or thousands of machines, it is expected that both enqueue and dequeue speed should be close to O(1) memory access, and that even with modest hardware Luxun can support hundreds of thousands of messages per second.</li>
<li><strong><em>Persistent and Durable</em></strong> : Real business value can be derived from big data, so any data lose should be avoided as far as possible. Also, nowadays backend system maintenance(or even crash) is common, Luxun should persist messages on disk longer than the maintenance(or system recovery) window, to let backend systems continue to consume messages when they are up again. Regarding durability, Luxun should ensure the persistence of the message even the service process crashes.</li>
<li><strong><em>Separation of Producers and Consumers</em></strong> : Luxun should separate messaging producers and consumers using pub-sub style exchange pattern, each one can work without knowing the existence of the others, such kind of loosely coupled architecture can make the whole system robust, horizontal scalable, and easy to maintain.</li>
<li><strong><em>Realtime</em></strong> : Messages produced by producer threads should be immediately visible to consumer threads, this feature is critical to event based system like Complex Event Processing(CEP) system.</li>
<li><strong><em>Distributed</em></strong> : Luxun should explicitly support partitioning messages over Luxun servers and distributing consumption over a cluster of consumer machines while maintaining per-partition ordering semantics.</li>
<li><strong><em>Multiple Client Support</em></strong> : Luxun system should support easy integration of clients from different kinds of platforms(such as Java, .Net, PHP, Ruby, Python, etc), it's desirable that producers and consumers can be auto-generated from Luxun service interface, by leveraging technology like Thrift RPC.</li>
<li><strong><em>Flexible consuming semantics</em></strong> : Luxun should support typical consume once queue, fanout queue, and provides more flexible consuming mechanism like consuming by index.</li>
<li><strong><em>Light Weight</em></strong> : The footprint of Luxun binary should be light, and the interface exposed should be simple and be understandable by normal user. Zookeeper like distributed coordination should be avoided since many small or medium scale companies still can't afford it, and the learning curve of zookeeker to average developers is still steep.</li>
</ul>
</blockquote>

<p>Luxun makes a unified big data platform(or pipeline) possible, as illustrated in the figure below:</p>

<p><img class="center" src="/images/luxun/arch-1.png" width="600" height="800"></p>

<p>The figure shows a typical big data collecting and analytics scenario supported by Luxun messaging system:<br/>
At the producing side, there are different kinds of producers, such as:</p>

<blockquote><ul>
<li>Frontend web applications producing application logs.</li>
<li>External tracking proxies producing web analytics logs.</li>
<li>Backend services producing service invocation trace logs.</li>
</ul>
</blockquote>

<p>At the consuming side, there are different kinds of consumers, such as:</p>

<blockquote><ul>
<li>Offline consumers consuming messages and storing them in Hadoop or traditional Data Warehouse for offline analysis.</li>
<li>Near realtime consumers consuming messages and store them in HBase or Cassandra for near realtime analytics.</li>
<li>Realtime consumers filter messages in in-memory DB and trigger alert events to related groups.</li>
</ul>
</blockquote>

<h1>Basic Concepts:</h1>

<blockquote><ol>
<li><strong><em>Topic</em></strong> : Logically it's a named place to send messages to or to consume messages from, physically it's a persistent queue.</li>
<li><strong><em>Broker</em></strong> : Aka Luxun server.</li>
<li><strong><em>Message</em></strong> : Datum to produce or consume</li>
<li><strong><em>Producer</em></strong> : A role which will send messages to topics.</li>
<li><strong><em>Consumer</em></strong> : A role which will consume messages from topics.</li>
<li><strong><em>Consumer Group</em></strong> : A group of consumers that will receive only one copy of a message from a topic(or more).</li>
</ol>
</blockquote>

<h1>Overall Architecture</h1>

<p><img class="center" src="/images/luxun/arch-2.png" width="600" height="800"></p>

<p>Luxun has a simple architecture, the main components of a broker are:</p>

<blockquote><ol>
<li><strong><em>Persistent Queue</em></strong> : Physical implementation of logic topic, internally use memory mapped file, automatic paging and swapping algorithm, sliding window, index based access for fast queue operation while use memory in an efficient way.</li>
<li><strong><em>Thrift based Interface</em></strong> : Simple RPC based API exposing queue service to external clients.</li>
<li><strong><em>Producer Client</em></strong> : Wrapper around Luxun producing API, provides simplified interface for developers, also provides advanced partitioning, batching, compression and asynchronous producing features.</li>
<li><strong><em>Consumer Client</em></strong> : Wrapper around Luxun consuming API, provides simplified and stream style consuming interface for developers, supporting advanced distributed consuming,  multi-threads concurrent consuming, group consuming features.</li>
<li><strong><em>Management and Monitoring</em></strong>: Server management and JMX based monitoring interface.</li>
</ol>
</blockquote>

<h1>The Core Principle</h1>

<p>The core principle of a fast while persistent queue system like Luxun is from a key observation that <strong><em><a href="http://queue.acm.org/detail.cfm?id=1563874">sequential disk read can be comparable to or even faster than random memory read</a></em></strong>, see a comparison figure below:</p>

<p><img class="center" src="/images/luxun/core_principle.png" width="600" height="800"></p>

<p>So if we can effectively organize the disk access pattern, then we can get fast performance comparable to memory which still have persistence. Queue is a rear append(or append only) and front read data structure, a nature fit to be implemented in sequential disk access mode.</p>

<h1>The Design of the Persistent Queue</h1>

<h3>Logical View</h3>

<p>The logic view of the persistent queue is fairly simple and intuitive, it's just like a big array, see figure below:</p>

<p><img class="center" src="/images/luxun/queue_logical_view.png" width="400" height="600"></p>

<p>you can access the queue using array like index, one special thing is that the index is of type long(in typical programming language, array index is of type int), so the queue can accomodate huge amount of data, only limited by available disk space. You may also think of the queue as a circular queue as shown in figure above, since the queue will wrap around when the long.max index is reached, (although in practice, we don't think current application will get chance to reach the long.max index:)).</p>

<p>With simple array like abstraction, we can implement queue semantics with ease:</p>

<blockquote><ol>
<li>For a typical consume once queue, we just need one rear pointer pointing to the queue rear index, aka the next to be appended index, another pointer pointing to the queue front index, aka the next to be consumed index. When an item is produced into the queue, we add the data in the queue then advance the rear index, when an item is consumed from the queue, we fetch the data in the queue then advance the front index. In this case, multi-threads can concurrently produce into the queue, the queue internally will sync the append operation, and multi-threads can concurrently consume(by contention) the queue, and every item will only be consumed by one thread once. see figure below.</li>
<li>For a fanout queue, we also just need one rear pointer pointing to the queue rear index, aka the next to be appended index, but on the consuming side, we let the queue maintain one queue front index for every fanout group, in other word, the fanout semantics is implemented in Luxun by letting Luxun server to maintain consuming state for every fanout group. In such case, multi-threads can concurrently and independently consume the queue, and every item will be consumed multiple times by different consumers as long as they belong to different consumer group(or fanout group). see figure below.</li>
</ol>
</blockquote>

<p><img class="center" src="/images/luxun/queue_semantics.png" width="400" height="600"></p>

<p>By the way, consume once queue is just a special case of fanout queue, so it's not necessary for luxun to provide a separate consume one queue, as long as fanout queue has been provided.</p>

<p>In summary, Luxun queue is an append only queue, means at producing side, item can only be appended into the queue, while at the consuming side, flexible queue consuming semantics are provided by array like index access model and state maintained on server side.</p>

<p>Note, the Luxun queue service even expose the index based queue access interface to user, in case some user may need more flexible queue semantics, for example, to support transactional queue semantics by committing and saving index in DB or Zookeeper. It's even possible to consume the queue randomly by index, although there may have performance issue in such case.</p>

<h3>Physical View</h3>

<p>Luxun queue is built on a big array abstraction, physically, one big array is implemented by:</p>

<blockquote><ul>
<li>One <strong><em>Index file</em></strong> : store fix sized index item, aka pointer to data item in <strong><em>Data File</em></strong>.</li>
<li>One <strong><em>Data file</em></strong> : store actual data item with variable length.</li>
</ul>
</blockquote>

<p>Index file and data file may grow very big, map whole index file or data file into memory may lead to unpredictable memory issue, so both Index file and Data file are further paged into fix sized sub-page files(in current setting, index page is 32M which can index 1M items, data page is 128M), and every sub-page has a corresponding index, at runtime, these sub-page files will be mapped into memory on demand.</p>

<p><img class="center" src="/images/luxun/queue_physical_view.png" width="700" height="900"></p>

<p>A fix sized <strong><em>Index Item</em></strong> consists of :</p>

<blockquote><ol>
<li>4 bytes <code>Data Page Index</code> - pointing to the data page index where the target data item resides.</li>
<li>2 bytes <code>Item Offset</code> - data item offset within the data page file.</li>
<li>2 bytes <code>Item Length</code> - the length of the data item.</li>
<li>4 bytes <code>Item Timestamp</code> - the timestamp when the data item is appended into the big array.</li>
</ol>
</blockquote>

<p>Besides structures above, every big array has :</p>

<blockquote><ol>
<li>An <strong><em>Array Header Index Pointer</em></strong> : pointing to the next to be appended array index.</li>
<li>An <strong><em>Array Tail Index Pointer</em></strong> : pointing to the first array index(usually it's 0 if we haven't truncated the array)</li>
<li>A <strong><em>Header Data Page Index Pointer</em></strong> : pointing to the next to be appended data page index.</li>
<li>A <strong><em>Header Data Item Offset Pointer</em></strong> : pointing to the next to be appended data item offset within a data page.  <br/>
Pointers 1 &amp; 2 are persisted in memory mapped file, while pointers 3 &amp; 4 are not persisted since they can be derived from pointers 1 &amp; 2.</li>
</ol>
</blockquote>

<p>With data and file structures defined above, let's see the simplified data item indexing and appending(producing) flow(we will use number listed above as abbreviated representation of pointer):</p>

<blockquote><ol>
<li>Find the header data page file through pointer 3.</li>
<li>Append the data into the data page file, starting offset from pointer 4, then update pointer 4 by adding the data length.</li>
<li>Find the header index item through pointer 1.</li>
<li>Update <code>Data Page Index</code>, <code>Item Offset</code>, <code>Item Length</code> and <code>Item Timestamp</code> within the index item.</li>
<li>Advance pointer 1 by plus one(this also has transactional commit effect).</li>
</ol>
</blockquote>

<p>The simplified reading(or consuming) by index flow is even simpler:</p>

<blockquote><ol>
<li>find the index item by the given index</li>
<li>find the data item by inspecting <code>Data Page Index</code>, <code>Item Offset</code> in the index item</li>
<li>read the data item and then return it.</li>
</ol>
</blockquote>

<p>Algorithm to find index item given an array index:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>index page index = array index / number of index items per page;
</span><span class='line'>index item offset = (array index &lt;code>mod&lt;/code> index items per page) * length of index item</span></code></pre></td></tr></table></div></figure></notextile></div>
The divide, mod and multiply operations are further optimized by fast shifting operations.</p>

<h4>Concurrency Consideration</h4>

<p>In the design above, the append operation must be synchronized in concurrent case, while read operation is thread safe, the <strong><em>Array Header Index Pointer</em></strong> is just like a read/writer barrier, one and only one append thread will push the barrier, while multiple threads can concurrently read items behind the barrier.</p>

<h3>Components View</h3>

<p><img class="center" src="/images/luxun/components_view.png" width="300" height="400"></p>

<p>Luxun persistent queue consists of following components:</p>

<blockquote><ol>
<li>At the top is the <strong><em>Fanout Queue</em></strong> abstraction layer, Luxun interacts with this layer directly for queue operations like enqueue and dequeue.</li>
<li><strong><em>Fanout Queue</em></strong> is built on the <strong><em>Append Only Big Array</em></strong> structure, just as we explained in the logical and physical view above.</li>
<li>The <strong><em>Append Only Big Array</em></strong> structure is built on <strong><em>Mapped Page Factory</em></strong> which are response for mapped page management, like page creation, on-demand load, caching, swapping, etc.</li>
<li>At the lowest level are the <strong><em>Mapped Page</em></strong> which is the object model of memory mapped file, and the <strong><em>LRU Cache</em></strong> which are responsible for the page cache and swapping, for efficient memory usage.</li>
</ol>
</blockquote>

<h3>Dynamic View</h3>

<p><img class="center" src="/images/luxun/sliding_window.png" width="600" height="800"></p>

<p>At runtime, Luxun queue looks just like a memory mapped sliding window:</p>

<blockquote><ol>
<li>As new items are appended into the queue, the queue rear index will slide gradually towards the right, and the current appended page will be mapped and kept in memory.</li>
<li>As items are read from the queue, the queue front index will slide gradually towards the right, and the current read page will be mapped and kept in memory.</li>
</ol>
</blockquote>

<p>Only the current active page files are mapped into memory, and they will be unloaded from memory if they are inactive in a specified time window, then new active page(s) will be loaded into memory as needed. The access pattern of queue((rear append and front read)) has very good locality, as long as we keep the current working set in memory, we'll obtain both fast read/write performance and persistence, while at the same time the memory usage is efficient.</p>

<h3>Other Design Decisions</h3>

<p>// TODO</p>

<p>The big array structure(aka the persistent queue) is implemented as a standalone library, since its usage it not limited to Luxun only, any applications that need a fast, big and persistent queue can reuse the big array library, the source of this library is <a href="https://github.com/bulldog2011/bigqueue">here</a>.</p>

<h1>Luxun vs Apache Kafka - the Main Differences</h1>

<p>Although Luxun borrowed many design ideas from Apache Kafka, Luxun is not a simple clone of Kafka, it has some obvious differentiating factors:</p>

<blockquote><ol>
<li>Luxun is based on <a href="http://en.wikipedia.org/wiki/Memory-mapped_file">Memory Mapped file</a>, while Kafka is based on filesystem and OS page cache, memory mapped file is a natural bridge between volatile memory and persistent disk, hence it will have better throughput, memory mapped file also has following unique features:

<ul>
<li>Message appended by producer thread will be immediately visible to consumer threads, even producer thread hasn't flushed the message explicitly, this makes realtime consuming possible.</li>
<li>OS will ensure the message persistence even the process crashes and there is no explicit flush before the crash.</li>
<li>In Java implementation, memory mapped file dose not use heap memory directly, so the GC impact is limited.</li>
</ul>
</li>
<li>Luxun leveraged <a href="http://thrift.apache.org/">Thrift RPC</a> as communication layer, while Kafka built its custom NIO communication layer and messaging protocol, custom NIO layer may have better performance, while Thrift makes generating communication infrastructure and cross-language clients(producer or consumer) fairy simple, this is a typical maintainability over performance design decision.</li>
<li>Luxun message consuming is index(array like) based, while Kafka message consuming is offset based, we believe index access mode can simplify design and can separate error domain better than offset.</li>
<li>Luxun uses simple and random distribution mechanism for scalability, similar to Kestrel, each server handles a set of reliable, ordered message queues. When you put a cluster of these server together, with no cross communication, and pick a server at random whenever you do a <code>produce</code> or <code>consume</code>, you end up with a reliable, loosely ordered message queue(in many situations, loose ordering is sufficient). On the other hand, Kafka relies on Zookeeper for distributed coordination, We believe Zookeeper is still too heavy-weight for small to medium sized companies(the main targets of Luxun), and the learning curve is still steep for average developers. Of cause, Luxun has extension point left for future possible Zookeeper integration.</li>
<li>Luxun only supports server level partitioning - partition a topic on different servers, while Kafka supports partitioning within a topic. Our performance test show partitioning within a topic has no performance gain, at the same time, it makes design complex.</li>
</ol>
</blockquote>

<p>The difference above is just difference, no one is better than the other, Luxun and Kafka have different architectural objectives,  different target user and applications.</p>

<h2>Contributions</h2>

<p>Luxun borrowed design ideas and adapted source from following open source projects:</p>

<blockquote><ol>
<li><a href="http://kafka.apache.org/index.html">Apache Kafka</a>, a distributed publish-subscribe messaging system using Scala as implementation language.</li>
<li><a href="https://github.com/adyliu/jafka">Jafka</a>, a Kafka clone using Java as implementation language.</li>
<li><a href="https://github.com/peter-lawrey/Java-Chronicle">Java Chronicel</a>, an ultra low latency, high throughput, persisted, messaging and event driven in memory database. using memory mapped file and index based access mode, implemented in Java.</li>
<li><a href="http://code.google.com/p/fqueue/">fqueue</a>, fast and persistent queue based on memory mapped file and paging algorithm, implemented in Java.</li>
<li><a href="http://code.google.com/p/ashes-queue/">ashes-queue</a>, FIFO queue based on memory mapped file and paging algorithm, implemented in Java.</li>
<li><a href="https://github.com/robey/kestrel">Kestrel</a>, a simple, distributed message queue system implemented in Scala, supporting reliable, loosely ordered message queue.</li>
</ol>
</blockquote>

<p>Many thanks to the authors of these open source projects!</p>
]]></content>
  </entry>
  
</feed>
