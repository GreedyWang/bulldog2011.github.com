<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: design | Abstraction Builder]]></title>
  <link href="http://bulldog2011.github.com/blog/categories/design/atom.xml" rel="self"/>
  <link href="http://bulldog2011.github.com/"/>
  <updated>2013-04-01T18:51:41+08:00</updated>
  <id>http://bulldog2011.github.com/</id>
  <author>
    <name><![CDATA[Bulldog]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[the architecture and design of a publish subscribe messaging system tailored for big data collecting and analytics]]></title>
    <link href="http://bulldog2011.github.com/blog/2013/03/27/the-architecture-and-design-of-a-pub-sub-messaging-system/"/>
    <updated>2013-03-27T16:30:00+08:00</updated>
    <id>http://bulldog2011.github.com/blog/2013/03/27/the-architecture-and-design-of-a-pub-sub-messaging-system</id>
    <content type="html"><![CDATA[<h1>Overview</h1>

<p>With the advent of big data era, we are facing more and more challenges from big data collecting and analytics requirements. Typical big data or activity stream includes but not limited to:</p>

<blockquote><ul>
<li>Logs generated by frontend applications or backend services</li>
<li>User behavior data</li>
<li>Application or system performance trace</li>
<li>Business, application or system metrics data.</li>
<li>Events that need immediate action.</li>
</ul>
</blockquote>

<p>The Luxun messaging system is just tailored for big data collecting and analytics scenario, following are the main design objectives of Luxun messaging system:</p>

<!--more-->


<blockquote><ul>
<li><strong><em>Fast and High-Throughput</em></strong> : This is the top priority feature, without this capability, the system will be easily overwhelmed by flooding data continuously generated by hundreds or thousands of machines, it is expected that both enqueue and dequeue speed should be close to O(1) memory access, and that even with modest hardware Luxun can support hundreds of thousands of messages per second.</li>
<li><strong><em>Persistent and Durable</em></strong> : Real business value can be derived from big data, so any data lose should be avoided as far as possible. Also, nowadays backend system maintenance(or even crash) is common, Luxun should persist messages on disk longer than the maintenance(or system recovery) window, to let backend systems continue to consume messages when they are up again. Regarding durability, Luxun should ensure the persistence of the message even the service process crashes.</li>
<li><strong><em>Separation of Producers and Consumers</em></strong> : Luxun should separate messaging producers and consumers using pub-sub style exchange pattern, each one can work without knowing the existence of the others, such kind of loosely coupled architecture can make the whole system robust, horizontal scalable, and easy to maintain.</li>
<li><strong><em>Realtime</em></strong> : Messages produced by producer threads should be immediately visible to consumer threads, this feature is critical to event based system like Complex Event Processing(CEP) system.</li>
<li><strong><em>Distributed</em></strong> : Luxun should explicitly support partitioning messages over Luxun servers and distributing consumption over a cluster of consumer machines while maintaining per-partition ordering semantics.</li>
<li><strong><em>Multiple Client Support</em></strong> : Luxun system should support easy integration of clients from different kinds of platforms(such as Java, .Net, PHP, Ruby, Python, etc), it's desirable that producers and consumers can be auto-generated from Luxun service interface, by leveraging technology like Thrift RPC.</li>
<li><strong><em>Flexible consuming semantics</em></strong> : Luxun should support typical consume once queue, fanout queue, and provides more flexible consuming mechanism like consuming by index.</li>
<li><strong><em>Light Weight</em></strong> : The footprint of Luxun binary should be light, and the interface exposed should be simple and be understandable by normal user. Zookeeper like distributed coordination should be avoided since many small or medium scale companies still can't afford it, and the learning curve of zookeeker to average developers is still steep.</li>
</ul>
</blockquote>

<p>Luxun makes a unified big data platform(or pipeline) possible, as illustrated in the figure below:</p>

<p><img class="center" src="/images/luxun/arch-1.png" width="600" height="800"></p>

<p>The figure shows a typical big data collecting and analytics scenario supported by Luxun messaging system:<br/>
At the producing side, there are different kinds of producers, such as:</p>

<blockquote><ul>
<li>Frontend web applications producing application logs.</li>
<li>External tracking proxies producing web analytics logs.</li>
<li>Backend services producing service invocation trace logs.</li>
</ul>
</blockquote>

<p>At the consuming side, there are different kinds of consumers, such as:</p>

<blockquote><ul>
<li>Offline consumers consuming messages and storing them in Hadoop or traditional Data Warehouse for offline analysis.</li>
<li>Near realtime consumers consuming messages and store them in HBase or Cassandra for near realtime analytics.</li>
<li>Realtime consumers filter messages in in-memory DB and trigger alert events to related groups.</li>
</ul>
</blockquote>

<h1>Basic Concepts:</h1>

<blockquote><ol>
<li><strong><em>Topic</em></strong> : Logically it's a named place to send messages to or to consume messages from, physically it's a persistent queue.</li>
<li><strong><em>Broker</em></strong> : Aka Luxun server.</li>
<li><strong><em>Message</em></strong> : Datum to produce or consume</li>
<li><strong><em>Producer</em></strong> : A role which will send messages to topics.</li>
<li><strong><em>Consumer</em></strong> : A role which will consume messages from topics.</li>
<li><strong><em>Consumer Group</em></strong> : A group of consumers that will receive only one copy of a message from a topic(or more).</li>
</ol>
</blockquote>

<h1>Overall Architecture</h1>

<p><img class="center" src="/images/luxun/arch-2.png" width="600" height="800"></p>

<p>Luxun has a simple architecture, the main components of a broker are:</p>

<blockquote><ol>
<li><strong><em>Persistent Queue</em></strong> : Physical implementation of logic topic, internally use memory mapped file, automatic paging and swapping algorithm, sliding window, index based access for fast queue operation while use memory in an efficient way.</li>
<li><strong><em>Thrift based Interface</em></strong> : Simple RPC based API exposing queue service to external clients.</li>
<li><strong><em>Producer Client</em></strong> : Wrapper around Luxun producing API, provides simplified interface for developers, also provides advanced partitioning, batching, compression and asynchronous producing features.</li>
<li><strong><em>Consumer Client</em></strong> : Wrapper around Luxun consuming API, provides simplified and stream style consuming interface for developers, supporting advanced distributed consuming,  multi-threads concurrent consuming, group consuming features.</li>
<li><strong><em>Management and Monitoring</em></strong>: Server management and JMX based monitoring interface.</li>
</ol>
</blockquote>

<h1>The Core Principle</h1>

<p>The core principle of a fast while persistent queue system like Luxun is from a key observation that <strong><em><a href="http://queue.acm.org/detail.cfm?id=1563874">sequential disk read can be comparable to or even faster than random memory read</a></em></strong>, see a comparison figure below:</p>

<p><img class="center" src="/images/luxun/core_principle.png" width="600" height="800"></p>

<p>So if we can effectively organize the disk access pattern, then we can get fast performance comparable to memory which still have persistence. Queue is a rear append(or append only) and front read data structure, a nature fit to be implemented in sequential disk access mode.</p>

<h1>The Design of the Persistent Queue</h1>

<h3>Logical View</h3>

<p>The logic view of the persistent queue is fairly simple and intuitive, it's just like a big array, see figure below:</p>

<p><img class="center" src="/images/luxun/queue_logical_view.png" width="400" height="600"></p>

<p>you can access the queue using array like index, one special thing is that the index is of type long(in typical programming language, array index is of type int), so the queue can accomodate huge amount of data, only limited by available disk space. You may also think of the queue as a circular queue as shown in figure above, since the queue will wrap around when the long.max index is reached(although in practice, we don't think current application will get chance to reach the long.max index:)).</p>

<p>With simple array like abstraction, we can implement queue semantics with ease:</p>

<blockquote><ol>
<li>For a typical consume once queue, we just need one rear pointer pointing to the queue rear index, aka the next to be appended index, another pointer pointing to the queue front index, aka the next to be consumed index. When an item is produced into the queue, we add the data in the queue then advance the rear index, when an item is consumed from the queue, we fetch the data in the queue then advance the front index. In this case, multi-threads can concurrently produce into the queue, the queue internally will sync the append operation, and multi-threads can concurrently consume(by contention) the queue, and every item will only be consumed by one thread once. see figure below.</li>
<li>For a fanout queue, we also just need one rear pointer pointing to the queue rear index, aka the next to be appended index, but on the consuming side, we let the queue maintain one queue front index for every fanout group, in other word, the fanout semantics is implemented in Luxun by letting Luxun server to maintain consuming state for every fanout group. In such case, multi-threads can concurrently and independently consume the queue, and every item will be consumed multiple times by different consumers as long as they belong to different consumer group(or fanout group). see figure below.</li>
</ol>
</blockquote>

<p><img class="center" src="/images/luxun/queue_semantics.png" width="400" height="600"></p>

<p>By the way, consume once queue is just a special case of fanout queue, so it's not necessary for luxun to provide a separate consume one queue, as long as fanout queue has been provided.</p>

<p>In summary, Luxun queue is an append only queue, means at producing side, item can only be appended into the queue, while at the consuming side, flexible queue consuming semantics are provided by array like index access model and state maintained on server side.</p>

<p>Note, the Luxun queue service even expose the index based queue access interface to user, in case some user may need more flexible queue semantics, for example, to support transactional queue semantics by committing and saving index in DB or Zookeeper. It's even possible to consume the queue randomly by index, although there may have performance issue in such case.</p>

<h3>Physical View</h3>

<p>Luxun queue is built on a big array abstraction, physically, one big array is implemented by:</p>

<blockquote><ul>
<li>One <strong><em>Index file</em></strong> : store fix sized index item, aka pointer to data item in <strong><em>Data File</em></strong>.</li>
<li>One <strong><em>Data file</em></strong> : store actual data item with variable length.</li>
</ul>
</blockquote>

<p>Index file and data file may grow very big, map whole index file or data file into memory may lead to unpredictable memory issue, so both Index file and Data file are further paged into fix sized sub-page files(in current setting, index page is 32M which can index 1M items, data page is 128M), and every sub-page has a corresponding index, at runtime, these sub-page files will be mapped into memory on demand.</p>

<p><img class="center" src="/images/luxun/queue_physical_view.png" width="700" height="900"></p>

<p>A fix sized <strong><em>Index Item</em></strong> consists of :</p>

<blockquote><ol>
<li>4 bytes <code>Data Page Index</code> - pointing to the data page index where the target data item resides.</li>
<li>2 bytes <code>Item Offset</code> - data item offset within the data page file.</li>
<li>2 bytes <code>Item Length</code> - the length of the data item.</li>
<li>4 bytes <code>Item Timestamp</code> - the timestamp when the data item is appended into the big array.</li>
</ol>
</blockquote>

<p>Besides structures above, every big array has :</p>

<blockquote><ol>
<li>An <strong><em>Array Header Index Pointer</em></strong> : pointing to the next to be appended array index.</li>
<li>An <strong><em>Array Tail Index Pointer</em></strong> : pointing to the first array index(usually it's 0 if we haven't truncated the array)</li>
<li>A <strong><em>Header Data Page Index Pointer</em></strong> : pointing to the next to be appended data page index.</li>
<li>A <strong><em>Header Data Item Offset Pointer</em></strong> : pointing to the next to be appended data item offset within a data page.  <br/>
Pointers 1 &amp; 2 are persisted in memory mapped file, while pointers 3 &amp; 4 are not persisted since they can be derived from pointers 1 &amp; 2.</li>
</ol>
</blockquote>

<p>With data and file structures defined above, let's see the simplified data item indexing and appending(producing) flow(we will use number listed above as abbreviated representation of pointer):</p>

<blockquote><ol>
<li>Find the header data page file through pointer 3.</li>
<li>Append the data into the data page file, starting offset from pointer 4, then update pointer 4 by adding the data length.</li>
<li>Find the header index item through pointer 1.</li>
<li>Update <code>Data Page Index</code>, <code>Item Offset</code>, <code>Item Length</code> and <code>Item Timestamp</code> within the index item.</li>
<li>Advance pointer 1 by plus one(this also has transactional commit effect).</li>
</ol>
</blockquote>

<p>The simplified reading(or consuming) by index flow is even simpler:</p>

<blockquote><ol>
<li>find the index item by the given index</li>
<li>find the data item by inspecting <code>Data Page Index</code>, <code>Item Offset</code> in the index item</li>
<li>read the data item and then return it.</li>
</ol>
</blockquote>

<p>Algorithm to find index item given an array index:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>index page index = array index / number of index items per page;
</span><span class='line'>index item offset = (array index &lt;code>mod&lt;/code> index items per page) * length of index item</span></code></pre></td></tr></table></div></figure></notextile></div>
The divide, mod and multiply operations are further optimized by fast shifting operations.</p>

<h4>Concurrency Consideration</h4>

<p>In the design above, the append operation must be synchronized in concurrent case, while read operation is thread safe, the <strong><em>Array Header Index Pointer</em></strong> is just like a read/writer barrier, one and only one append thread will push the barrier, while multiple threads can concurrently read items behind the barrier.</p>

<h3>Components View</h3>

<p><img class="center" src="/images/luxun/components_view.png" width="300" height="400"></p>

<p>Luxun persistent queue consists of following components:</p>

<blockquote><ol>
<li>At the top is the <strong><em>Fanout Queue</em></strong> abstraction layer, Luxun interacts with this layer directly for queue operations like enqueue and dequeue.</li>
<li><strong><em>Fanout Queue</em></strong> is built on the <strong><em>Append Only Big Array</em></strong> structure, just as we explained in the logical and physical view above.</li>
<li>The <strong><em>Append Only Big Array</em></strong> structure is built on <strong><em>Mapped Page Factory</em></strong> which are responsible for mapped page management, like page creation, on-demand load, caching, swapping, etc.</li>
<li>At the lowest level are the <strong><em>Mapped Page</em></strong> which is the object model of memory mapped file, and the <strong><em>LRU Cache</em></strong> which are responsible for the page cache and swapping, for efficient memory usage.</li>
</ol>
</blockquote>

<h3>Dynamic View</h3>

<p><img class="center" src="/images/luxun/sliding_window.png" width="600" height="800"></p>

<p>At runtime, Luxun queue looks just like a memory mapped sliding window:</p>

<blockquote><ol>
<li>As new items are appended into the queue, the queue rear index will slide gradually towards the right, and the current appended page will be mapped and kept in memory.</li>
<li>As items are read from the queue, the queue front index will slide gradually towards the right, and the current read page will be mapped and kept in memory.</li>
</ol>
</blockquote>

<p>Only the current active page files are mapped into memory, and they will be unloaded from memory if they are inactive in a specified time window, then new active page(s) will be loaded into memory as needed. The access pattern of queue((rear append and front read)) has very good locality, as long as we keep the current working set in memory, we'll obtain both fast read/write performance and persistence, while at the same time the memory usage is efficient.</p>

<h3>Other Design Consideration</h3>

<h4>The Queue Interface</h4>

<p>The persistent queue exposes following interfaces for queue operations, monitoring and maintenance:</p>

<blockquote><ol>
<li><strong><em>enqueue(byte[] item)</em></strong> : append(or produce) binary data into the queue.</li>
<li><strong><em>byte[] dequeue</em></strong> : read(or consume) binary data from the queue.</li>
<li><strong><em>isEmpty</em></strong> : check if the queue is empty.</li>
<li><strong><em>getSize</em></strong> : total number of items remaining in the queue</li>
<li><strong><em>removeBefore(long timestamp)</em></strong> : remove index and data pages before a given timestamp, this may delete back files and advance the queue front pointer accordingly. This interface is useful to remove expired data files, to clean up already consumed data or to avoid too much disk space being used up. Luxun supports a <code>log.retention.hours</code> setting, internally, Luxun will periodically check data files outside the retention window, and remove them using the <code>removeBefore</code> interface.</li>
<li><strong><em>limitBackFileSize(long sizeLimit)</em></strong> : limit the total size of back index and data page files, this may delete back files and advance the queue front pointer accordingly. This interface is useful to limit the total size of back files, to clean up already consumed data or to avoid too much disk space being used up.  Luxun supports a <code>log.retention.size</code> setting, internally, Luxun will periodically check the total size of back files and use <code>limitBackFileSize</code> interface to prune some old back files to maintain size.</li>
<li><strong><em>getBackFileSize</em></strong> : Get current total size of back files of a queue.</li>
<li><strong><em>findClosestIndex(long timestamp)</em></strong> : find index closest to a given timestamp, this interface is useful in some cases that user want to consume by index and from a specific timestamp.</li>
<li><strong><em>flush</em></strong> : force to persist newly appended data, usually, this interface is not needed since OS will be responsible for the persistence of memory mapped buffer. We leave this interface to user in case they may need transactional reliability and they are aware of the cost to performance.</li>
</ol>
</blockquote>

<p>Note, in our queue design, enqueue only accept binary data as input, and dequeue only returns binary data, serialization and deserialization are outside the consideration of big queue design, in other word, Luxun can only see plain and raw bytes, we choose this design because:</p>

<blockquote><ol>
<li>This can simplify the queue design and implementation.</li>
<li>There are already a couple of mature and high performance serialization frameworks out there, such as protobuf, thrift, avro, just name a few, We can't do better than these already established frameworks.</li>
<li>We give the flexibility to user to choose their preferred serialization framework.</li>
</ol>
</blockquote>

<h4>Reusability</h4>

<p>The big array structure(aka the persistent queue) is implemented as a standalone library, since its usage it not limited to Luxun only, any applications that need a fast, big and persistent queue can reuse the big array library, the source of this library is <a href="https://github.com/bulldog2011/bigqueue">here</a>.</p>

<h1>The Design of Thrift based Communication Layer</h1>

<h3>Rationality</h3>

<p>We choosed Thrift to implement the communication layer of Luxun, because:</p>

<blockquote><ol>
<li>Thrift is <strong><em>stable and mature</em></strong>, it was created by Facebook, now it's an Apache project, it has been successfully used by famous projects like Cassandra and HBase.</li>
<li>Thrift has <strong><em>high performance</em></strong>, it provides highly effective serialization protocols like <code>TBinaryProtocol</code> and server model like <code>TNonBlockingServer</code>(so you won't get troubled with building your own NIO server which is very tricky and error prone). High performance binary RPC support and non blocking server model are main attractive features that lead us to choose Thrift, since <strong><em>Fast Wire Serialization, High Throughput, Highly Concurrent and NonBlocking Communication</em></strong> are top priority architecture and design objectives of Luxun.</li>
<li>Thrift is <strong><em>simple and light-weight</em></strong>, you just need to define a simple interface using its light-weight IDL(interface definition language), then you can auto-generate basic server and client proxy code without much effort, this can not only minimize development effort, but later upgrading effort - you just need to update the IDL then re-generate.</li>
<li>Thrift has good <strong><em>cross-language</em></strong> support, supported platforms include but not limited to Java, CSharp, C++, PHP, Ruby, Python. One big factor we choose Thrift is - after we build the Thrift based Luxun queue service, clients for different language platforms are basically ready, If We need a client for languge X, We can easily generate one using its universal code generator.</li>
<li>Thrift is <strong><em>flexible</em></strong>, Thrift has a pluggable architecture, transport protocols(like tcp or http), serialization protocol(like TBinaryProtocol, TJSONProtocol) and server models(like TNonBlockingServer, TThreadPoolServer) are all changeable according to your real needs.</li>
</ol>
</blockquote>

<p>Basically, We think guys at Facebook have made a really cool RPC framework, greatly simplified service development.</p>

<h3>Components View</h3>

<p>Programming with Thrift just like playing with building blocks, see components view of the Luxun client and server below:</p>

<p><img class="center" src="/images/luxun/communication_components.png" width="600" height="800"></p>

<blockquote><ol>
<li>At the lowest layer is the underlying IO supported by Java language platform.</li>
<li>In the middle layer are components provided by Thrift, in Luxun implementation,
on server side, we chosen:

<ul>
<li><code>TnonblockingServerSocket</code> as <code>TTransport</code> protocol;</li>
<li><code>TBinaryProtocol</code> as <code>TProtocol</code> serialization protocol;</li>
<li>and <code>TNonblockingServer</code> as server model.<br/>
on client side, we chosen:</li>
<li><code>TSocket</code> as <code>TTransport</code> protocol;</li>
<li><code>TBinaryProtocol</code> as <code>TProtocol</code> serialization protocol.</li>
</ul>
</li>
<li>The <code>QueueService.Processor</code> and <code>QueueService.Client</code> are proxy auto-generated from Luxun service IDL, will be elaborated on later.</li>
<li>On top layer are the Luxun implementation of the queue service interface, on server side, we implement <code>QueueService.Iface</code> in a class called <code>LogManager</code>, which will communicate with clients through the generated <code>QueueService.Processor</code> proxy and delegate the real queue operations to the underlying persistent queue; On client side, we implement producer or consumer specific code, which communicates with the server through the generated <code>QueueService.Client</code> proxy.</li>
</ol>
</blockquote>

<h3>Luxun Thrift IDL</h3>

<p>The Luxun Thrift IDL is the messaging contract between Luxun server and clients(producers or consumers), see its formal definition below:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
<span class='line-number'>100</span>
<span class='line-number'>101</span>
<span class='line-number'>102</span>
<span class='line-number'>103</span>
<span class='line-number'>104</span>
<span class='line-number'>105</span>
<span class='line-number'>106</span>
<span class='line-number'>107</span>
<span class='line-number'>108</span>
<span class='line-number'>109</span>
<span class='line-number'>110</span>
<span class='line-number'>111</span>
<span class='line-number'>112</span>
<span class='line-number'>113</span>
<span class='line-number'>114</span>
<span class='line-number'>115</span>
<span class='line-number'>116</span>
<span class='line-number'>117</span>
<span class='line-number'>118</span>
<span class='line-number'>119</span>
<span class='line-number'>120</span>
<span class='line-number'>121</span>
<span class='line-number'>122</span>
<span class='line-number'>123</span>
<span class='line-number'>124</span>
<span class='line-number'>125</span>
<span class='line-number'>126</span>
<span class='line-number'>127</span>
<span class='line-number'>128</span>
<span class='line-number'>129</span>
<span class='line-number'>130</span>
<span class='line-number'>131</span>
<span class='line-number'>132</span>
<span class='line-number'>133</span>
<span class='line-number'>134</span>
<span class='line-number'>135</span>
<span class='line-number'>136</span>
<span class='line-number'>137</span>
<span class='line-number'>138</span>
<span class='line-number'>139</span>
<span class='line-number'>140</span>
<span class='line-number'>141</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>namespace java com.leansoft.luxun.api.generated
</span><span class='line'>namespace csharp Leansoft.luxun.Api.Generated&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>const i32 RANDOM_PARTION = -1;&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>const string EARLIEST_INDEX_STRING = "earliest";
</span><span class='line'>const string LATEST_INDEX_STRING = "latest";&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>const i64 EARLIEST_TIME = -1;
</span><span class='line'>const i64 LATEST_TIME = -2;&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>enum ResultCode
</span><span class='line'>{
</span><span class='line'>  SUCCESS,
</span><span class='line'>  FAILURE,
</span><span class='line'>  TRY_LATER
</span><span class='line'>}&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>enum ErrorCode
</span><span class='line'>{
</span><span class='line'>  INTERNAL_ERROR,
</span><span class='line'>  TOPIC_NOT_EXIST,
</span><span class='line'>  INDEX_OUT_OF_BOUNDS,
</span><span class='line'>  INVALID_TOPIC,
</span><span class='line'>  TOPIC_IS_EMPTY,
</span><span class='line'>  AUTHENTICATION_FAILURE,
</span><span class='line'>  MESSAGE_SIZE_TOO_LARGE,
</span><span class='line'>  ALL_MESSAGE_CONSUMED
</span><span class='line'>}&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>struct Result
</span><span class='line'>{&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>1: required ResultCode resultCode,
</span><span class='line'>2: ErrorCode errorCode,
</span><span class='line'>3: string errorMessage
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p>}&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>struct ProduceRequest {&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>1: required binary item,
</span><span class='line'>2: required string topic,
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p>}&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>struct ProduceResponse {&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>1: required Result result,
</span><span class='line'>2: i64 index
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p>}&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>struct ConsumeRequest {&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>1: required string topic,
</span><span class='line'>2: string fanoutId, 
</span><span class='line'>3: i64 startIndex,
</span><span class='line'>4: i32 maxFetchSize,
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p>}&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>struct ConsumeResponse {&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>1: required Result result,
</span><span class='line'>2: list&lt;binary&gt; itemList,
</span><span class='line'>3: i64 lastConsumedIndex
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p>}&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>struct FindClosestIndexByTimeRequest {&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>1: required i64 timestamp,
</span><span class='line'>2: required string topic,
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p>}&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>struct FindClosestIndexByTimeResponse {&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>1: required Result result,
</span><span class='line'>2: i64 index,
</span><span class='line'>3: i64 timestampOfIndex
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p>}&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>struct DeleteTopicRequest {&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>1: required string topic,
</span><span class='line'>2: required string password
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p>}&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>struct DeleteTopicResponse {&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>1: required Result result,
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p>}&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>struct GetSizeRequest {&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>1: required string topic,
</span><span class='line'>3: string fanoutId
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p>}&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>struct GetSizeResponse {&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>1: required Result result,
</span><span class='line'>2: i64 size
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p>}&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>service QueueService {&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>ProduceResponse produce(1: ProduceRequest produceRequest);
</span><span class='line'>
</span><span class='line'>ConsumeResponse consume(1: ConsumeRequest consumeRequest);
</span><span class='line'>
</span><span class='line'>FindClosestIndexByTimeResponse findClosestIndexByTime(1: FindClosestIndexByTimeRequest findClosestIndexByTimeRequest);
</span><span class='line'>
</span><span class='line'>DeleteTopicResponse deleteTopic(1: DeleteTopicRequest deleteTopicRequest);
</span><span class='line'>
</span><span class='line'>GetSizeResponse getSize(1: GetSizeRequest getSizeRequest);
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p>}&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p></span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>This is a quite simple and intuitive interface, let's elaborate on supported calls one by one:</p>

<blockquote><ol>
<li>The <strong><em>produce</em></strong> call append binary data into the queue, you need to provide the target topic and the binary data as input, the response will return the operation result and the appended index if the operation is successful.</li>
<li>The <strong><em>consume</em></strong> call supports to kinds of operation modes,

<ul>
<li><strong><em>consume by fanout id</em></strong>: this is just the fanout queue semantics support, in such mode, you need to provide target topic, a fanout id and a max fetch size as input, the response will return a list of binary data if the operation is successful.</li>
<li><strong><em>consume by index</em></strong>: this is a more flexible queue semantics support, in such mode, you need to provide target topic, a start index and a max fetch size as input, the response will return a list of binary data if the operation is successful.    <br/>
Note, if both fanout id and start index are provided, then fanout id will take precedence.
The max fetch size parameter is required to support batch consuming - one consume operation can fetch data up to the max fetch size, then return the whole batch list of binary data, this can improve consuming throughput a lot. If you just need to consume one item at a time, set max fetch size to &lt;= 0;</li>
</ul>
</li>
<li>The <strong><em>findClosestIndexByTime</em></strong> call is useful if you want <strong><em>consume by index</em></strong> semantics and want to find an index by a specific timestamp. You need to provide a target topic and a timestamp as input, the response will return closest index if the find operation is successful.</li>
<li>The <strong><em>deleteTopic</em></strong> call is used for deleting any unused topics, you need to provide a target topic and a authentication password(set on server side) as input, the response will return operation result, this is a call for queue administration.</li>
<li>The <strong><em>getSize</em></strong> call just returns the total number of items remaining in a topic, this is a call for queue status query, if fanout id is provided, then the size of specific fanout queue will be returned, if no fanout id is provided, then the size of underlying queue(big array) will be returned.</li>
</ol>
</blockquote>

<p>Simplicity is the ultimate design objective of the Luxun queue IDL, in order to simplify clients implementation and to make the interface understandable by average developer, at the sample, future extension is easy because of the flexibility and IDL driven development provided by Thrift.</p>

<h1>The Design of the Producer</h1>

<p>The raw producer client generated from Luxun queue IDL can be used directly in real application, however, we believe the raw client is too low level for most average developers, so we provided a high-level and feature rich client which is actually a wrapper around the low level raw client generated from IDL. The high level producer not only provides a simpler and intuitive interface for average application developers, but provides advanced features like partitioning, compression, asynchronous batching, further improving the message producing performance. Let's see the main design elements of the producer below.</p>

<h3>The Interface</h3>

<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>IProducer.java </span><a href='https://github.com/bulldog2011/luxun/blob/master/src/main/java/com/leansoft/luxun/producer/IProducer.java'>source  </a></figcaption> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;/&lt;</span><span class="n">em</span><span class="o">&gt;*</span>
</span><span class='line'> <span class="o">*</span> <span class="n">Producer</span> <span class="kd">interface</span>
</span><span class='line'> <span class="err">*</span>
</span><span class='line'> <span class="o">*</span> <span class="nd">@author</span> <span class="n">bulldog</span>
</span><span class='line'> <span class="o">*</span> <span class="nd">@param</span> <span class="o">&lt;</span><span class="n">K</span><span class="o">&gt;</span> <span class="n">partition</span> <span class="n">key</span>
</span><span class='line'> <span class="o">*</span> <span class="nd">@param</span> <span class="o">&lt;</span><span class="n">V</span><span class="o">&gt;</span> <span class="n">real</span> <span class="n">message</span>
</span><span class='line'> <span class="o">*</span>
</span><span class='line'> <span class="o">&lt;/</span><span class="n">em</span><span class="o">&gt;/</span>
</span><span class='line'><span class="kd">public</span> <span class="kd">interface</span> <span class="nc">IProducer</span><span class="o">&amp;</span><span class="n">lt</span><span class="o">;</span><span class="n">K</span><span class="o">,</span> <span class="n">V</span><span class="o">&gt;</span> <span class="kd">extends</span> <span class="n">Closeable</span> <span class="o">{&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="cm">/**</span>
</span><span class='line'><span class="cm"> * Send messages</span>
</span><span class='line'><span class="cm"> * </span>
</span><span class='line'><span class="cm"> * @param data message data</span>
</span><span class='line'><span class="cm"> * @throws NoBrokersForTopicException no broker for this topic</span>
</span><span class='line'><span class="cm"> */</span>
</span><span class='line'><span class="kt">void</span> <span class="nf">send</span><span class="o">(</span><span class="n">ProducerData</span><span class="o">&amp;</span><span class="n">lt</span><span class="o">;</span><span class="n">K</span><span class="o">,</span> <span class="n">V</span><span class="o">&amp;</span><span class="n">gt</span><span class="o">;</span> <span class="n">data</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">NoBrokersForTopicException</span><span class="o">;</span>
</span><span class='line'>
</span><span class='line'><span class="cm">/**</span>
</span><span class='line'><span class="cm"> * get message encoder</span>
</span><span class='line'><span class="cm"> * </span>
</span><span class='line'><span class="cm"> * @return message encoder</span>
</span><span class='line'><span class="cm"> * @see Encoder</span>
</span><span class='line'><span class="cm"> */</span>
</span><span class='line'><span class="n">Encoder</span><span class="o">&amp;</span><span class="n">lt</span><span class="o">;</span><span class="n">V</span><span class="o">&amp;</span><span class="n">gt</span><span class="o">;</span> <span class="n">getEncoder</span><span class="o">();</span>
</span><span class='line'>
</span><span class='line'><span class="cm">/**</span>
</span><span class='line'><span class="cm"> * get partition chooser</span>
</span><span class='line'><span class="cm"> * </span>
</span><span class='line'><span class="cm"> * @return partition chooser</span>
</span><span class='line'><span class="cm"> * @see IPartitioner</span>
</span><span class='line'><span class="cm"> */</span>
</span><span class='line'><span class="n">IPartitioner</span><span class="o">&amp;</span><span class="n">lt</span><span class="o">;</span><span class="n">K</span><span class="o">&amp;</span><span class="n">gt</span><span class="o">;</span> <span class="n">getPartitioner</span><span class="o">();</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;}&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>ProducerData.java </span><a href='https://github.com/bulldog2011/luxun/blob/master/src/main/java/com/leansoft/luxun/producer/ProducerData.java'>source  </a></figcaption> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;/&lt;</span><span class="n">em</span><span class="o">&gt;*</span>
</span><span class='line'><span class="o">*</span> <span class="n">Represents</span> <span class="n">the</span> <span class="n">data</span> <span class="n">to</span> <span class="n">be</span> <span class="n">sent</span> <span class="n">using</span> <span class="n">the</span> <span class="n">Producer</span> <span class="n">send</span> <span class="n">API</span>
</span><span class='line'><span class="o">*</span>
</span><span class='line'><span class="o">*</span> <span class="nd">@author</span> <span class="n">bulldog</span>
</span><span class='line'><span class="o">*</span> <span class="nd">@param</span><span class="o">&lt;</span><span class="n">K</span><span class="o">&gt;</span> <span class="n">partition</span> <span class="n">key</span>
</span><span class='line'><span class="o">*</span> <span class="nd">@param</span><span class="o">&lt;</span><span class="n">V</span><span class="o">&gt;</span> <span class="n">real</span> <span class="n">data</span>
</span><span class='line'><span class="o">*</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">em</span><span class="o">&gt;/</span>
</span><span class='line'><span class="kd">public</span> <span class="kd">class</span> <span class="nc">ProducerData</span><span class="o">&amp;</span><span class="n">lt</span><span class="o">;</span><span class="n">K</span><span class="o">,</span> <span class="n">V</span><span class="o">&gt;</span> <span class="o">{&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="cm">/** the topic under which the message is to be published */</span>
</span><span class='line'><span class="kd">private</span> <span class="n">String</span> <span class="n">topic</span><span class="o">;</span>
</span><span class='line'>
</span><span class='line'><span class="cm">/** the key used by the partitioner to pick a broker */</span>
</span><span class='line'><span class="kd">private</span> <span class="n">K</span> <span class="n">key</span><span class="o">;</span>
</span><span class='line'>
</span><span class='line'><span class="cm">/** variable length data to be published as Luxun messages under topic */</span>
</span><span class='line'><span class="kd">private</span> <span class="n">List</span><span class="o">&amp;</span><span class="n">lt</span><span class="o">;</span><span class="n">V</span><span class="o">&amp;</span><span class="n">gt</span><span class="o">;</span> <span class="n">data</span><span class="o">;</span>
</span><span class='line'>
</span><span class='line'><span class="o">.</span>
</span><span class='line'><span class="o">.</span>
</span><span class='line'><span class="o">.</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;}&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>This interface is quite self explanatory, a real <code>IProducer</code> implementation is <a href="https://github.com/bulldog2011/luxun/blob/master/src/main/java/com/leansoft/luxun/producer/Producer.java">here</a>, to publish data to Luxun server, you just call <code>send</code> with target topic and data(or a list of data) as input, optionally, you may :</p>

<blockquote><ul>
<li>Provide an <a href="https://github.com/bulldog2011/luxun/blob/master/src/main/java/com/leansoft/luxun/serializer/Encoder.java">Encoder</a> to let the producer know how to convert your data into binary format,</li>
<li>Provide an <a href="https://github.com/bulldog2011/luxun/blob/master/src/main/java/com/leansoft/luxun/producer/IPartitioner.java">IPartitioner</a> to let the producer know how to choose the target Luxun borker, in such case, you may also provide a partition key as part of the data.</li>
</ul>
</blockquote>

<h3>Partitioning</h3>

<p>At producer side, Luxun supports distribution through client side partitioning - the producer will choose a target broker using the default or user specified partitioner, if the default <a href="https://github.com/bulldog2011/luxun/blob/master/src/main/java/com/leansoft/luxun/producer/DefaultPartitioner.java">random partitioner</a> is used, then the producer will pick a server at random when producing data, this end up with a distributed queue that each server stands alone and is strongly ordered, making the cluster loosely ordered.
In many situations, loose ordering is sufficient. Dropping the requirement on cross communication makes it horizontally scale to infinity and beyond: no multicast, not "elections", no coordination at all.</p>

<p><img class="center" src="/images/luxun/partitioning.png" width="400" height="600"></p>

<p>It's also feasible to use a VIP between the producers and the Luxun cluster, in such case, producers only need to know the address of VIP, the VIP will be responsible for distributing traffic to different brokers.</p>

<p><img class="center" src="/images/luxun/vip.png" width="400" height="600"></p>

<h3>Compression</h3>

<p>Big data is big consumer of network bandwidth and disk storage, compression, if used appropriately, can
reduce bandwidth and storage usage to an acceptable level. Most of big data like logs are text data, making compression a high priority choice when designing the collecting system.</p>

<p>In current implementation, Luxun supports <a href="https://en.wikipedia.org/wiki/Gzip">gzip</a> and <a href="http://en.wikipedia.org/wiki/Snappy_%28software%29">snappy</a> compression, enabled by producer side setting. Below is the message encoding flow at producer side:</p>

<blockquote><ol>
<li>Use user specified or default <code>Encoder</code> to convert user provided data into binary format then wrap them in a TMessageList container object, TMessageList is generated from Thrift IDL, see definition <a href="https://github.com/bulldog2011/luxun/blob/master/src/main/resources/thrift/message.thrift">here</a>.</li>
<li>Convert the TMessageList into binary format using Thrift serialization, compress the binary according to user specified codec, if no codec specified, then the binary data won't be compressed.</li>
<li>Wrap the compressed binary data with codec into another TMessagePack container object, TMessagePack is also generated from Thrift IDL, see definition <a href="https://github.com/bulldog2011/luxun/blob/master/src/main/resources/thrift/message.thrift">here</a>.</li>
<li>Convert the TMessagePack into binary format using Thrift serialization, then send the binary to Luxun broker by calling <code>produce</code> raw API.</li>
</ol>
</blockquote>

<p>Below is corresponding message decoding flow at consumer side:</p>

<blockquote><ol>
<li>Get binary data from Luxun broker by calling <code>consume</code> raw API.</li>
<li>Convert the binary data into a TMessagePack object by using Thrift serialization.</li>
<li>Extract codec and the wrapped binary data from the TMessagePack object, decompress the binary data according to the codec.</li>
<li>Convert the decompressed binary data into a TMessageList object by using Thrift serialization.</li>
<li>Extract data wrapped in the TMessageList object and convert them into user format by using user specified or default <code>Decoder</code>.</li>
</ol>
</blockquote>

<h3>Batch &amp; Asynchronous Producing</h3>

<p>The roundtrip overhead of RPC call over network has a significant impact on the system throughput and performance, as a best practice, many big data systems use batch and asynchronous producing technology at producer side for higher throughput, our testing also showed that the performance of batch producing is far better(order of magnitude differences) than the performance of producing without batch.</p>

<p>Figures below vividly show the inner working of synchronous and asynchronous(aka batch) producing:</p>

<p><img class="center" src="/images/luxun/sync_producer.png" width="400" height="600"></p>

<p><img class="center" src="/images/luxun/async_producer.png" width="400" height="600"></p>

<p>First, in both sync and async producing modes, there are a couple of sync(or async) producers cached on the producer side, and there is a one to one mapping between a sync(or async) producer and a Luxun broker, at runtime, whenever there is a producing request, the producer will pick one concrete producer instance from cache by partition policy .</p>

<p>In sync producer mode, the sync producer will send a message to Luxun broker everytime it gets a message sending request from calling threads, and the calling thread will block before the <code>send</code> call return.</p>

<p>In asyn producer mode, messages from sending threads will be cached in an inner blocking queue first, and there is an inner sender thread which will continuously poll messages from the queue, collate and pack messages into chunk according to target topic, and send the chunk to Luxun server eventually. In async mode, message sending is actually triggered by <strong><em>either</em></strong> of following configurable conditions:</p>

<blockquote><ol>
<li>The number of messages in the blocking queue has reached a threshhold.</li>
<li>A time interval has expired.</li>
</ol>
</blockquote>

<p>Async producing does not block calling thread, the calling thread just fire the message then forget, so async producing has no performance impact on upper layer calling application, it's a preferred mode for most big data collecting scenarios.</p>

<p>Although the throughput of sync producing does not compare with async producing, in some situations, sync producing is a preferred mode, for example, in real time event system, speed is a top priority  while throughput is a secondary consideration.</p>

<h1>Luxun vs Apache Kafka - the Main Differences</h1>

<p>Although Luxun borrowed many design ideas from Apache Kafka, Luxun is not a simple clone of Kafka, it has some obvious differentiating factors:</p>

<blockquote><ol>
<li>Luxun is based on <a href="http://en.wikipedia.org/wiki/Memory-mapped_file">Memory Mapped file</a>, while Kafka is based on filesystem and OS page cache, memory mapped file is a natural bridge between volatile memory and persistent disk, hence it will have better throughput, memory mapped file also has following unique features:

<ul>
<li>Message appended by producer thread will be immediately visible to consumer threads, even producer thread hasn't flushed the message explicitly, this makes realtime consuming possible.</li>
<li>OS will ensure the message persistence even the process crashes and there is no explicit flush before the crash.</li>
<li>In Java implementation, memory mapped file dose not use heap memory directly, so the GC impact is limited.</li>
</ul>
</li>
<li>Luxun leveraged <a href="http://thrift.apache.org/">Thrift RPC</a> as communication layer, while Kafka built its custom NIO communication layer and messaging protocol, custom NIO layer may have better performance, while Thrift makes generating communication infrastructure and cross-language clients(producer or consumer) fairy simple, this is a typical maintainability over performance design decision.</li>
<li>Luxun message consuming is index(array like) based, while Kafka message consuming is offset based, we believe index access mode can simplify design and can separate error domain better than offset.</li>
<li>Luxun uses simple and random distribution mechanism for scalability, similar to Kestrel, each server handles a set of reliable, ordered message queues. When you put a cluster of these server together, with no cross communication, and pick a server at random whenever you do a <code>produce</code> or <code>consume</code>, you end up with a reliable, loosely ordered message queue(in many situations, loose ordering is sufficient). On the other hand, Kafka relies on Zookeeper for distributed coordination, We believe Zookeeper is still too heavy-weight for small to medium sized companies(the main targets of Luxun), and the learning curve is still steep for average developers. Of cause, Luxun has extension point left for future possible Zookeeper integration.</li>
<li>Luxun only supports server level partitioning - partition a topic on different servers, while Kafka supports partitioning within a topic. Our performance test show partitioning within a topic has no performance gain, at the same time, it makes design complex.</li>
</ol>
</blockquote>

<p>The difference above is just difference, no one is better than the other, Luxun and Kafka have different architectural objectives,  different target user and applications.</p>

<h1>Performance</h1>

<h2>Contributions</h2>

<p>Luxun borrowed design ideas and adapted source from following open source projects:</p>

<blockquote><ol>
<li><a href="http://kafka.apache.org/index.html">Apache Kafka</a>, a distributed publish-subscribe messaging system using Scala as implementation language.</li>
<li><a href="https://github.com/adyliu/jafka">Jafka</a>, a Kafka clone using Java as implementation language.</li>
<li><a href="https://github.com/peter-lawrey/Java-Chronicle">Java Chronicel</a>, an ultra low latency, high throughput, persisted, messaging and event driven in memory database. using memory mapped file and index based access mode, implemented in Java.</li>
<li><a href="http://code.google.com/p/fqueue/">fqueue</a>, fast and persistent queue based on memory mapped file and paging algorithm, implemented in Java.</li>
<li><a href="http://code.google.com/p/ashes-queue/">ashes-queue</a>, FIFO queue based on memory mapped file and paging algorithm, implemented in Java.</li>
<li><a href="https://github.com/robey/kestrel">Kestrel</a>, a simple, distributed message queue system implemented in Scala, supporting reliable, loosely ordered message queue.</li>
</ol>
</blockquote>

<p>Many thanks to the authors of these open source projects!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[wsdl driven development on iOS - the big picture]]></title>
    <link href="http://bulldog2011.github.com/blog/2013/03/25/wsdl-driven-development-on-ios-the-big-picture/"/>
    <updated>2013-03-25T20:03:00+08:00</updated>
    <id>http://bulldog2011.github.com/blog/2013/03/25/wsdl-driven-development-on-ios-the-big-picture</id>
    <content type="html"><![CDATA[<p>WSDL driven development is is a popular and mature development methodology on platforms like Java and .Net, tools like Axis, CXF, JAX-WS, WCF are used by many developers for rapid web service development. With WSDL as interface contract, both server side and client side proxy can be automatically generated from WSDL, developer can work with plain old interfaces/objects directly, no need to worry about low level SOAP/XML serialization/deserialization and invocation details(which are very tedious and error-prone), this kind of model driven development(or meta-data driven development) can not only significantly reduce initial development cost, but reduce long term maintenance cost.</p>

<!--more-->


<p>Although nowadays there is a trend toward RESTful service(which has no formal interface definition like WSDL), many industries(such as ecommerce industry) have a complex business domain, it's very hard to expose complex business logic as RESTful service(a typical maintenance nightmare), so we will keep seeing that many enterprises will keep exposing their services as traditional SOAP or XML based web services, some examples are <a href="https://affiliate-program.amazon.com/gp/advertising/api/detail/main.html">Amazon Product Advertising Web Service</a>, eBay <a href="https://www.x.com/developers/ebay/products/finding-api">Finding</a>, <a href="https://www.x.com/developers/ebay/products/shopping-api">Shopping</a> and <a href="https://www.x.com/developers/ebay/products/trading-api">Trading</a> Web Services, etc.</p>

<p>Can WSDL driven development be put into practice on iOS platform? Yes, now with <a href="https://github.com/bulldog2011/pico">Pico Web Service Client Framework</a> and <a href="https://github.com/bulldog2011/mwsc">WSDL compiler for iOS</a>, you can also leverage WSDL driven development technology on iOS platform, tremendously improving your application development speed.</p>

<p>Let's see the big picture:</p>

<p><img class="center" src="/images/pico/big_picture.png" width="600" height="800"></p>

<p>The picture above is the blueprint of WSDL driven development on iOS. The left part of the blueprint is a build-time phase view, in this phase, we will leverage <a href="https://github.com/bulldog2011/mwsc">mwsc</a> wsdl compiler to automatically generate service proxy from wsdl, the service proxy alone can't be used directly, it must be integrated with the generic <a href="https://github.com/bulldog2011/pico">Pico Web Service client framework</a> to take effect; The right part of the blueprint is a runtime view, a typical flow starts from your iOS app, your app issues request on service proxy, the proxy passes the request to the Pico runtime framework which will delegate the object to XML/SOAP marshalling work to Pico binding framework then send XML/SOAP request to external service through AFNetworking HTTP client component, when an XML/SOAP response is received by the HTTP client component, the Pico runtime framework will also delegate the XML/SOAP to object unmarshalling work to Pico binding framework and passes the response object to the proxy which will eventually return the response object back to the calling app.</p>

<p>We used the synchronous flow in the picture just for convenient demonstration, the real call flow in the runtime is asynchronous, by leveraging NSOperation, NSOperationQueue and GCD, in order not to block main UI thread.</p>

<p>The Pico binding framework is the core of the Pico runtime, the real object<->xml binding magic happens here, below is the architecture of the binding runtime:</p>

<p><img class="center" src="/images/pico/binding.png" width="500" height="600"></p>

<p>There are four main components:</p>

<blockquote><ol>
<li><strong><em>Marshaller</em></strong> - responsible for object to xml marshalling</li>
<li><strong><em>Unmarshaller</em></strong> - responsible for xml to object unmarshalling</li>
<li><strong><em>BindingSchema</em></strong> - store object<->xml mapping information, used by both Marshaller and Unmarshaller to guide the marshalling/unmarshalling process at runtime, schema is extracted from classes and is cached for better performance.</li>
<li><strong><em>Converter</em></strong> - type converter for primitive types or frequently used types.</li>
</ol>
</blockquote>

<p>The marshalling/unmarshalling algorithm is recursive in nature:</p>

<blockquote><ol>
<li>for fields of primitive or frequently used types, use corresponding converter to convert the field directly.</li>
<li>for fields of object type, convert the fields of the object one by one and recursively.</li>
</ol>
</blockquote>

<p>The code generator component is based on <a href="http://docs.oracle.com/javase/6/docs/technotes/tools/share/wsimport.html">JAX-WS Wsimport</a> and <a href="http://freemarker.org/">Freemarker</a> template engine, see the whole architecture below:</p>

<p><img class="center" src="/images/pico/codegen-arch.png" width="600" height="750"></p>

<p>The whole architecture can be summarized as <strong><em>Model + Template = Code</em></strong>, code generation flow as following:</p>

<blockquote><ol>
<li>WSDL doc is first fed into Wsimport and a JAXB/JAX-WS model is generated.</li>
<li>The JAXB/JAX-WS model is then transformed into a language independent intermediate codegen model.</li>
<li>The codegen model and corresponding target language templates(Objective-C or Android) are then fed into the Freemarker template engine.</li>
<li>The Freemarker template engine will eventually transform the in-memory model into target code, guided by the templates.</li>
</ol>
</blockquote>

<p>There are other WSDL to Objective-C code generation tools, like <a href="http://code.google.com/p/wsdl2objc/">wsdl2objc</a> and <a href="http://sudzc.com/">SudzC</a>, it's ok to use these tools to generate code from simple wsdl, but when fed with an industry level wsdl like eBay or Amazon wsdl, seems these tools will always break. Unlike these simple tools, the mwsc code generator provided by Pico framework is backed by JAX-WS Wsimport, which is mature and stable, and can recognize most standard WSDL/Schema components, by delegating the most tricky and complex wsdl model transformation task to Wsimport, the mwsc code generator solved the WSDL to Objective-C(or Android Java) problem in a simple while elegant way.</p>

<p>The mwsc code generator not only generates simple bean classes from WSDL/Schema, but also generates the XML<->object mapping information(schema) and record these information in the class as meta-data, the meta-data will later be leveraged by Pico binding framework to guide the XML<->Object transformation at runtime.</p>

<p>With a generic code generation tool and a generic web service client runtime, web service based app development on iOS platform becomes easy, there is no low level xml parsing and http handling(which are tedious, error-prone and hard to maintain) any more, developers only need to work with a plain old service interface for service invocation, now they can put their real effort on application logic and UI, leading to agile iOS app development.</p>

<p>In later posts, I will show how to put WSDL driven iOS app development into practice in a series of tutorials, just stay tuned.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Design of A Big, Fast, and Persistent Queue]]></title>
    <link href="http://bulldog2011.github.com/blog/2013/01/23/big-queue-design/"/>
    <updated>2013-01-23T20:55:00+08:00</updated>
    <id>http://bulldog2011.github.com/blog/2013/01/23/big-queue-design</id>
    <content type="html"><![CDATA[<h1>Why a Big Queue?</h1>

<p>This is a big data era, we are always facing challenge to find insights in big data. Last time, I have worked on the architecture and design of a large scale logging, tracing, monitoring and analytics platform, the core of the platform is a log collecting system and the core of the collector is a big queue, see figure below:</p>

<!-- more -->


<p><img class="center" src="/images/bigqueue/log_collector.png" width="400" height="400"></p>

<p>The figure above looks like a typical producing and consuming scenario, the big queue works like a broker, at the left side of the queue, there are many agents deployed on application servers, the agents work just like producers, they continuously collect log data on application servers and push them to the big queue, on the right side of the big queue, there are several analysis systems, the analysis systems work just like consumers, they continuously pull log data from the queue, analyze  and store them into the backend. If you are interested in an industrial log collecting product built on this architecture, please refer to <a href="http://kafka.apache.org/">apache kafka</a>.</p>

<p>Queue is a natural fit for log data collecting scenario, by leveraging queue, consumers and producers are decoupled, both sides can work without knowing the existing of other side, consumers and producers can be added or removed as needed without affecting existing ones.</p>

<h1>The Requirements</h1>

<p>Basically, we need the big queue to be:</p>

<ul>
<li><strong>FAST &amp; THREAD SAFE</strong>    <br/>
The system needs to collect logs from more than 1000 production machines, they may produce more than 100,000 logs per second(average size is 1KB), this is equal to more than 100MB per second, if the big queue cant keep up, logs will be lost. To further improve the throughout, we want all producers and consumers to work concurrently, so the queue needs to work in a thread safe manner, otherwise, there will be data lose or corruption.</li>
<li><strong>BIG &amp; PERSISTENT</strong> <br/>
Daily logs will be at TB level, the queue should have the capacity to store up to one weeks logs,
In case any of the analysis system is down(for example, periodical maintenance or even crash), the queue should continue to store logs for backend system to consume later when they are up again. Also, if the big queue itself is down, the logs already stored should survive since they are persistent, when the queue is up again, it should continue to serve the consumers.</li>
<li><strong>MEMORY EFFICIENT</strong><br/>
Compared with big disk storage, current computer system is still shortage of physical memory, usually, memory will be less than 32GB on a commodity machine. We need the queue to use memory efficiently though it needs to handle logs more than 100MB per second and at TB level daily.</li>
</ul>


<h1>The Design Thinking Flow</h1>

<p>Below is a simple and elegant design of the big queue I come up with to meet the requirements and challenges above:</p>

<p><img class="center" src="/images/bigqueue/bigqueue_abstraction_layers.png" width="300" height="300"></p>

<p>Usually, when I design something, I follow a top-down abstract thinking flow: as I learned in data structure course in college long ago, a queue data structure is usually built on an array data structure, so before I can build a queue I need to build an array first, the array I need should have following characters: first, it should be as fast as in memory access; second, it should be disk backed(hence it will be big and persistent). Seems there is contradiction between these two characters: if you need something fast, you need to put it in memory which is volatile and only has limited capacity, if you need something big and persistent, you need to put it on disk which has much slower access speed than physical memory, is there a technology to resolve these two contradictions? After an intensive research, I finally found memory mapped file which seems a natural bridge between psychical memory and disk, if you need background about memory mapped file, <a href="http://www.kdgregory.com/index.php?page=java.byteBuffer">here</a> and <a href="http://vanillajava.blogspot.co.uk/2012/03/presentation-on-using-shared-memory-in.html">here</a> are two good references. Now lets continue the top-down thinking flow, before I can build a big array, I need to build a data structure called memory mapped page which can bridge the gap between speed and capacity, at the same time, I need some auxiliary structures to manage mapped pages in a memory efficient way, in the design figure above, these auxiliary structures are called mapped page factory and LRU cache. Whenever big array needs a mapped page, it requests one from the mapped page factory and returns it to the factory when it has done with the page. Mapped page factory encapsulates algorithm to allocate, cache and recycle mapped pages in a memory efficient and thread safe way by leveraging LUR cache structure.</p>

<p>Now, with the design in mind, I can implement these abstract structures in a bottom up, layer by layer approach, you can find the implementation details by studying the open source java code on <a href="https://github.com/bulldog2011/bigqueue">github</a> if you are interested.</p>

<h1>Additional Design Notes:</h1>

<ul>
<li><p>Although I learned some people used to map a single big file into memory, like <a href="http://kdgcommons.svn.sourceforge.net/viewvc/kdgcommons/trunk/src/main/java/net/sf/kdgcommons/buffer/MappedFileBuffer.java?revision=HEAD&amp;view=markup">here</a> and <a href="http://vanillajava.blogspot.com/2011/12/using-memory-mapped-file-for-huge.html">here</a>, I have memory leak concern with such approach(though I am not sure), instead, I came with up a novel pagging and swapping algorithm which only map fixed size(for example, 128M) page file into memory on demand and unmap it when it is not accessed within a fixed time to live(TTL) period. Which such design, I can not only use memory safer and more efficient, but can delete some used pages files to save disk space whenever necessary.</p></li>
<li><p>As we know, queue is a rear append and front read structure, so as long as the queue front page and rear page(technically, this is called working set) are in memory, read and append operations can always happen in memory, that means the enqueue and dequeue operations are always close to O(1) memory access.</p></li>
<li><p>The big queue is based on a <strong>big array</strong> structure, the big array itself is an interesting data structure with some unique features(I plan to write some use cases of this structure in the near future), the big array supports sequential append(called append only array), sequential and random read. Sequential append and read are both O(1) memory access, while random read is O(1) memory access if the corresponding page is in cache, and is O(1) disk access if the corresponding page is not in cache. The big array is index based, just like normal indexed array, starting with index 0, when a new item is appended, the head index will be incremented, index is the pointer to the appended data, later you can use the index to read back the data. The index is of type long, this is a really very big range, I guess the big array wont be used up before the world is end:).</p></li>
<li><p>Internally, two logical files(phycially one logical file consists of many fixed size page files) are used by one big array, one is index file, the other is data file, when data is read by index, the index will be first mapped to an index page file, then into an item in the index page file, the index item has pointer and length information to the actual data in data file, data can be retrieved by just inspecting index item, load corresponding data page file and read data in it. New data can be appended just by finding out next to be appended data page file and offset, then put the data and update corresponding index item.</p></li>
<li><p>Serialization is outside of the consideration of the big queue framework, the enqueue operation only accepts byte array as input(the dequeue operation only returns byte array), I left out serialization deliberately since I think it should not be the responsibility of the big queue framework, there are many existing and well known serialization frameworks(like protobuf, thrift, etc) which can do serialization work better.</p></li>
<li><p>To ensure thread safe, some multi-threading technologies like read-write lock and thread local buffer are leveraged, the queue can work in read/write separation way  consumer and producer can work concurrently, this tremendously improved the throughput of the queue.</p></li>
<li><p>The queue has interface to delete used page files(for example, if data in these page files have been consumed by consumers) to save disk space, this is called garbage collection on disk files, much like GC in memory.</p></li>
<li><p>Abstractly, the whole queue looks like a huge FIFO circle buffer, disk backed and memory mapped.</p></li>
</ul>


<h1>Performance Test</h1>

<p>Below is the preformance test conclusion:</p>

<ul>
<li>In concurrent producing and consuming case, the average throughput is around <strong><em>166MBps</em></strong>.</li>
<li>In sequential producing then consuming case, the average throughput is around <strong><em>333MBps</em></strong>.</li>
</ul>


<p>Suppose the average message size is 1KB, then big queue can concurrently producing and consuming<br/>
166K message per second on a commodity machine under normal load. Basically, the throughput is only limited by disk IO bandwidth.</p>

<p>The detailed performance test report is <a href="https://github.com/bulldog2011/bigqueue/wiki/Performance-Test-Report">here</a>, the corresponding test program is <a href="https://github.com/bulldog2011/bigqueue/blob/master/src/test/java/com/leansoft/bigqueue/perf/BigQueuePerfTest.java">here</a>, and the full hardware spec for benchmark is <a href="http://bulldog2011.github.com/lab/">here</a>.</p>

<h1>Conclusion</h1>

<p>To resolve a big data challenge I designed and implemented a simple while elegant big queue that is:</p>

<blockquote><ol>
<li><strong>Fast</strong> : close to the speed of direct memory access, both enqueue and dequeue are close to O(1) memory access.</li>
<li><strong>Big</strong> : the total size of the queue data is only limited by the available disk space.</li>
<li><strong>Persistent</strong> : all data in the queue is persisted on disk, and is crash resistant.</li>
<li><strong>Memory-efficient</strong> : automatic paging &amp; swapping algorithm, only most-recently accessed data is kept in memory.</li>
<li><strong>Thread-safe</strong>: multiple threads can concurrently enqueue and dequeue without data corruption.</li>
<li><strong>Simple and Light-weight</strong>: current number of source files is 12 and the library jar is less than 20K.</li>
</ol>
</blockquote>

<p>Log data collecting is just use case of the big queue, I can anticipate the big queue will be used in more scenarios since big data challenges are becoming common these days.</p>
]]></content>
  </entry>
  
</feed>
